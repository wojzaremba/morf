\section{Experiments}

We are going to present results on test time performance of approximation with monochromatic filters, and bi-clustering. Moreover,
we additionally support our approximations with visualizations. Major contribution is in lowered evaluation time, while kept prediction performance.
Table \ref{evaluation_time} gives us reference on prediction time for network without approximations.

\begin{table}[t]
\tiny
\parbox{.99\linewidth}{
\centering
\begin{tabular}{rrrrr}
\hline
& Evaluation & & Evaluation &  \\
Layer & Time & Fraction & Time per img. & Fraction \\
& (bs = 1) & (bs = 1) & (bs = 128) & (bs = 128) \\
\hline
Conv1 & & & & \\
MaxPool & & & & \\
LRNormal & & & & \\
Conv2 & & & & \\
MaxPool & & & & \\
LRNormal & & & & \\
Conv3 & & & & \\
MaxPool & & & & \\
Conv4 & & & & \\
Conv5 & & & & \\
MaxPool & & & & \\
FC & & & & \\
FC & & & & \\
FC & & & & \\
Softmax & & & & \\
\hline 
Total & & & & \\
\hline
\end{tabular}
\vspace{5mm}
}
\parbox{.99\linewidth}{
\centering
\begin{tabular}{rrrrr}
\hline
& Evaluation & & Evaluation &  \\
Layer & Time & Fraction & Time per img. & Fraction \\
& (bs = 1) & (bs = 1) & (bs = 128) & (bs = 128) \\
\hline
Conv1 & $277.52 \pm 22.51$ & 6.46\% & $21.57 \pm 2.33$ & 21.11\% \\
MaxPool & $12.96 \pm 3.15$ & 0.30\% & $1.01 \pm 0.13$ & 0.99\% \\
LRNormal & $1.49 \pm 1.03$ & 0.03\% & $1.80 \pm 0.11$ & 1.76\% \\
Conv2 & $978.16 \pm 108.69$ & 22.80\% & $33.61 \pm 2.87$ & 32.88\% \\
MaxPool & $9.07 \pm 0.09$ & 0.21\% & $0.74 \pm 0.10$ & 0.72\% \\
LRNormal & $1.48 \pm 2.34$ & 0.03\% & $0.79 \pm 0.09$ & 0.78\% \\
Conv3 & $589.82 \pm 51.11$ & 13.74\% & $14.32 \pm 0.75$ & 14.01\% \\
MaxPool & $3.61 \pm 0.30$ & 0.08\% & $0.58 \pm 0.05$ & 0.57\% \\
Conv4 & $930.85 \pm 93.65$ & 21.69\% & $12.15 \pm 0.67$ & 11.89\% \\
Conv5 & $1211.67 \pm 60.15$ & 28.24\% & $11.40 \pm 0.61$ & 11.15\% \\
MaxPool & $0.81 \pm 0.07$ & 0.01\% & $0.11 \pm 0.01$ & 0.10\% \\
FC & $224 \pm 8.72$ & 5.22\% & $3.19 \pm 0.34$ & 3.12\% \\
FC & $37.01 \pm 8.31$ & 0.86\% & $0.65 \pm 0.04$ & 0.63\% \\
FC & $9.24 \pm 2.15$ & 0.21\% & $0.17 \pm 0.02$ & 0.17\% \\
Softmax & $1.78 \pm 4.16$ & 0.04\% & $0.04 \pm 0.05$ & 0.04\%\\
\hline 
Total & 4289.73 & & 102.20 & \\
\hline
\end{tabular}
}
\caption{Evaluation time (in ms) per layer on CPU for (top) AlexNet, (bottom) MattNet. We consider real time application setting (batch size aka bs of 1), and
mass scale annotation (batch size of 128). Results are averaged over 8 trails.} 
\label{evaluation_time}
\end{table}



\subsection{Monochromatic Filters}
Monochromatic approximation can work well only if color components span few one dimensional subspaces. 
Figure \ref{components}, \ref{components} show that it is indeed case for both AlexNet, as
well as MattNet. In case of AlexNet, we colored with a different colors every feature map, so one dimensional structure
is clearly depicted. For MattNet, we found further low-dimensional structure. All the colors seems to lay close
to two planes. Visualization gives different color to color components laying on every plane. Moreover,
colors span one dimensional subspaces within planes. We further confirmed that monochromatic approximations
are faithful. 

Figures \ref{denoising} and \ref{denoising} shows first layer filters after approximating them 
with monochromatic filters. This corresponds to the projection on one out of ''number of clusters`` 1-dim plane (line).
One can notice that often approximated filters look cleaner than original one.


\begin{figure}[t]
\mbox{
  \subfigure{
      \includegraphics[width=0.45\linewidth]{img/color_components_alex_3d.eps}
  }\quad
\subfigure{
  \includegraphics[width=0.45\linewidth]{img/color_components_matthew_3d.eps} 
  }
}
\label{components}
\caption{Visualization of color components for (left) AlexNet and (right) MattNet.}
\end{figure}


\begin{figure}[t]
\mbox{
  \subfigure{
      \includegraphics[width=0.45\linewidth]{img/denoising_alex.png}
  }\quad
\subfigure{
  \includegraphics[width=0.45\linewidth]{img/denoising_matthew.png} 
  }
}
\label{denoising}
\caption{Left column depicts original filters, while the right one approximated one with 12 clusters for (left) AlexNet and (right) MattNet (good to look in color). }
\end{figure}


\subsubsection{Reduction in memory overhead}
\begin{figure}[t]
\centering
\mbox{
  \includegraphics[width=0.75\linewidth]{img/monochromatic_numcolors_vs_numweights.eps} 
}
\label{monochromatic_numweights}
\caption{Relative savings in number of weights required for first convolutional layer for various monochromatic approximations applied to MattNet (red) and AlexNet (blue).}
\end{figure}

\subsection{Bi-clustering}

\subsubsection{Reduction in memory overhead}
\begin{figure}[t]
\centering
\mbox{
  \includegraphics[width=0.75\linewidth]{img/biclustering_rank_vs_numweights_matt.eps} 
}
\label{biclustering_numweights}
\caption{Relative decrease in number of weights required for second convolutional layer for various bi-clustering approximations applied to MattNet.}
\end{figure}
